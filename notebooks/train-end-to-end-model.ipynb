{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c81b3b-ca8d-426d-bd5d-7437f34c6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "os.environ[\"PYTHONUTF8\"]=\"1\"\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd360c33-0907-4f9c-a316-d514cf304cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b987edd-f322-420b-82c6-9066b29d6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prefect\n",
    "from prefect import task, Flow, Parameter\n",
    "from prefect.run_configs import LocalRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f23cf-a29d-4264-87e9-09369e177738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7716434-6979-4cc8-b492-bd117a267df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "from datetime import datetime\n",
    "import random\n",
    "import codecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c634d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07cb106",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DOWNLOAD = False\n",
    "_FE = False\n",
    "_TRAIN = False\n",
    "_PREFECT_FLOW = True\n",
    "_VERTEX = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8a73f-4e26-4423-bb29-1b6b391e4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_convert_for_wandb(artifact, path, profile=True):\n",
    "    \n",
    "    artifact.add_dir(path, name=\"data\")\n",
    "\n",
    "    for file_name in os.listdir(path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            path_to_file = os.path.join(path, file_name)\n",
    "            tab_name = file_name.replace(\".csv\", \"\")\n",
    "            print(f\"adding {tab_name}\")\n",
    "            df = pd.read_csv(path_to_file)\n",
    "            print(f\"{tab_name}:{df.shape}\")\n",
    "            table = wandb.Table(dataframe=df)\n",
    "            artifact.add(table, name=tab_name)\n",
    "            \n",
    "            if profile:\n",
    "                #The output of the profile report will be an HTML which we will log to W&B under the artifact made\n",
    "                data_profile = ProfileReport(df, dark_mode=True, title=tab_name, minimal=True)\n",
    "                profile_path = f\"{tab_name}.html\"\n",
    "                data_profile.to_file(profile_path)\n",
    "                data_table_profile = wandb.Html(profile_path)\n",
    "                artifact.add(data_table_profile, f\"{tab_name}_profile\")\n",
    "                # artifact.add_file(profile_path)\n",
    "                \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab26715c-3385-4613-9872-74337155e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(log_stdout=True)\n",
    "def download_and_log_kaggle_data(competition: str = \"tabular-playground-series-mar-2022\", project_name: str = \"kaggle-tps-mar-2022-odsc\"):\n",
    "\n",
    "    logger = prefect.context.get(\"logger\")\n",
    "\n",
    "    print(f\"starting new run for {project_name}\")\n",
    "    run = wandb.init(\n",
    "        project=project_name, job_type=\"download\", name=f\"log-{competition}\")\n",
    "\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.competition_download_files(competition)\n",
    "    zip_path = f\"{competition}.zip\"\n",
    "    path_to_raw = os.path.join(\".\", \"data\", \"raw\")\n",
    "    ZipFile(zip_path).extractall(path=path_to_raw)\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    # TODO: Remove hack to add data secription\n",
    "    if competition == \"tabular-playground-series-mar-2022\":\n",
    "        data_description = \"\"\"\n",
    "            In this competition, you'll forecast twelve-hours of traffic flow in a major U.S. metropolitan area. Time, space, and directional features give you the chance to model interactions across a network of roadways.\n",
    "\n",
    "            Files and Field Descriptions\n",
    "            -------------------------------\n",
    "            train.csv - the training set, comprising measurements of traffic congestion across 65 roadways from April through September of 1991.\n",
    "            row_id - a unique identifier for this instance\n",
    "            time - the 20-minute period in which each measurement was taken\n",
    "            x - the east-west midpoint coordinate of the roadway\n",
    "            y - the north-south midpoint coordinate of the roadway\n",
    "            direction - the direction of travel of the roadway. EB indicates \"eastbound\" travel, for example, while SW indicates a \"southwest\" direction of travel.\n",
    "            congestion - congestion levels for the roadway during each hour; the target. The congestion measurements have been normalized to the range 0 to 100.\n",
    "            test.csv - the test set; you will make hourly predictions for roadways identified by a coordinate location and a direction of travel on the day of 1991-09-30.\n",
    "            sample_submission.csv - a sample submission file in the correct format\n",
    "        \"\"\"\n",
    "\n",
    "    raw_data_artifact = wandb.Artifact(\n",
    "        name=\"raw\", type=competition, description=data_description)\n",
    "    add_convert_for_wandb(raw_data_artifact, path_to_raw)\n",
    "\n",
    "    run.log_artifact(raw_data_artifact)\n",
    "    run.finish()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373b9eb-d158-40fb-a940-15496f3da1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _DOWNLOAD:\n",
    "    download_and_log_kaggle_data.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664a211-1289-4253-bb8d-965ab3282429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    \n",
    "#     data = raw_data.copy(deep=True)\n",
    "    \n",
    "    data['time'] = pd.to_datetime(data['time'])\n",
    "    data['month'] = data['time'].dt.month\n",
    "    data['weekday'] = data['time'].dt.weekday\n",
    "    data['hour'] = data['time'].dt.hour\n",
    "    data['minute'] = data['time'].dt.minute\n",
    "    data['is_month_start'] = data['time'].dt.is_month_start.astype('int')\n",
    "    data['is_month_end'] = data['time'].dt.is_month_end.astype('int')\n",
    "    data['hour+minute'] = data['time'].dt.hour * 60 + data['time'].dt.minute\n",
    "    data['is_weekend'] = (data['time'].dt.dayofweek > 4).astype('int')\n",
    "    data['is_afternoon'] = (data['time'].dt.hour > 12).astype('int')\n",
    "    data['x+y'] = data['x'].astype('str') + data['y'].astype('str')\n",
    "    data['x+y+direction'] = data['x'].astype('str') + data['y'].astype('str') + data['direction'].astype('str')\n",
    "    data['hour+direction'] = data['hour'].astype('str') + data['direction'].astype('str')\n",
    "    data['hour+x+y'] = data['hour'].astype('str') + data['x'].astype('str') + data['y'].astype('str')\n",
    "    data['hour+direction+x'] = data['hour'].astype('str') + data['direction'].astype('str') + data['x'].astype('str')\n",
    "    data['hour+direction+y'] = data['hour'].astype('str') + data['direction'].astype('str') + data['y'].astype('str')\n",
    "    data['hour+direction+x+y'] = data['hour'].astype('str') + data['direction'].astype('str') + data['x'].astype('str') + data['y'].astype('str')\n",
    "    data['hour+x'] = data['hour'].astype('str') + data['x'].astype('str')\n",
    "    data['hour+y'] = data['hour'].astype('str') + data['y'].astype('str')\n",
    "#     data = data.drop(['time'], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4561e3-1505-492b-b1df-31cf5155830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(log_stdout=True)\n",
    "def feature_engineer_log_tps_2022(competition: str = \"tabular-playground-series-mar-2022\", project_name: str = \"kaggle-tps-mar-2022-odsc\"):\n",
    "    run = wandb.init(\n",
    "        project=project_name, job_type=\"feature_engineer\", name=f\"feature_engineer-{competition}\")\n",
    "    comp_data_art = run.use_artifact(f\"{project_name}/raw:latest\", type=competition)\n",
    "    comp_data_path = os.path.join(comp_data_art.download(), \"data\")\n",
    "    \n",
    "    train_path = os.path.join(comp_data_path, \"train.csv\")\n",
    "    test_path = os.path.join(comp_data_path, \"test.csv\")\n",
    "    submission_path = os.path.join(comp_data_path, \"sample_submission.csv\")\n",
    "    \n",
    "    train_data = pd.read_csv(train_path, dtype={'time': str})\n",
    "    test_data = pd.read_csv(test_path, dtype={'time': str})\n",
    "    submission = pd.read_csv(submission_path)\n",
    "\n",
    "    fe_train_data = feature_engineering(train_data)\n",
    "    fe_test_data = feature_engineering(test_data)\n",
    "    \n",
    "    local_data_dir = os.path.join(\"..\", \"data\")\n",
    "    fe_path = os.path.join(local_data_dir, \"fe\")\n",
    "    if not os.path.exists(fe_path):\n",
    "        os.makedirs(fe_path)\n",
    "    \n",
    "    fe_train_data_path = os.path.join(fe_path, \"fe_train.csv\")\n",
    "    fe_test_data_path = os.path.join(fe_path, \"fe_test.csv\")\n",
    "    \n",
    "    fe_train_data.to_csv(fe_train_data_path, index=False)\n",
    "    fe_test_data.to_csv(fe_test_data_path, index=False)\n",
    "    \n",
    "    fe_artifact = wandb.Artifact(\n",
    "        name=\"feature_engineered\", type=competition)\n",
    "    add_convert_for_wandb(fe_artifact, fe_path)\n",
    "    \n",
    "    run.log_artifact(fe_artifact)\n",
    "    run.finish()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc2f6c-42bc-4e43-af51-f49e23e61d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _FE:\n",
    "    feature_engineer_log_tps_2022.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022a6a1-c215-4fc3-8b90-1f4618ef4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessory function to call the collection of functions needed to convert useful information from the pycaret run into loggable artifacts for lineaging\n",
    "def perform_experiment(exp):\n",
    "\n",
    "    # Experiments are run by splitting a data into training and holdout internally, allowing their ability to make comparison\n",
    "\n",
    "    # Runs an experiment which will compare different model types here and select the best model type\n",
    "    best_model = compare_models()\n",
    "\n",
    "    # Return the dataframe that shows the different metrics calculated for each of the tested model types\n",
    "    leaderboard = get_leaderboard()\n",
    "    # Get the internal names of the models for referential ID's in a DataFrame\n",
    "    available_model_types = models()\n",
    "    # Merge the above Dataframes\n",
    "    model_comparison_results = leaderboard.reset_index().merge(available_model_types.reset_index(), left_on=\"Model Name\", right_on=\"Name\")\n",
    "\n",
    "    # Takes the best model type from above and fine tune it to find the best hyperparameters, \n",
    "    # and collect useful information about the model during the tuning process\n",
    "    tuned_finalized_model, tuned_model_results, tuner_cv_results = tune_and_finalize_model_with_metrics(best_model)\n",
    "    return model_comparison_results, tuned_finalized_model, tuned_model_results, tuner_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d760c-dc5d-4ef4-bfa9-e0b3dfd00e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take a PyCaret model and generate an optimized model and the results of the optimization steps in dataframes\n",
    "def tune_and_finalize_model_with_metrics(model):\n",
    "    tuned_model, tuner = tune_model(model, return_tuner=True)\n",
    "    #Pull collects the latest calculated table from output/experimentation into a dataframe\n",
    "    #The pull after a tune_model call will return the details of optimization steps over a variety of metrics\n",
    "    tuned_model_results = pull().reset_index()\n",
    "    tuned_model_results[\"index\"] = tuned_model_results[\"index\"].astype(str)\n",
    "\n",
    "    #The tuner cv results will return scores and more internal details of the model as it was tested over the optimization search schema\n",
    "    tuner_cv_results = pd.DataFrame(tuner.cv_results_).reset_index()\n",
    "    tuner_cv_results[\"index\"]  = tuner_cv_results[\"index\"].astype(str)\n",
    "\n",
    "    #We finalize the model to train over the whole dataset (no holdout/validation dataset split)\n",
    "    tuned_finalized_model = finalize_model(tuned_model)\n",
    "    return tuned_finalized_model, tuned_model_results, tuner_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87128654-3151-439c-8314-22a74f4c629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pycaret.readthedocs.io/en/latest/api/regression.html\n",
    "#TODO: Hardcode the relationships between the features and the numeric vs categorical features\n",
    "def setup_tps_2022_config(seed):\n",
    "    config = {\n",
    "        \"target\": \"congestion\",\n",
    "        \"fold_strategy\" : 'timeseries',\n",
    "        \"session_id\": seed,\n",
    "        \"ignore_features\" : [\"row_id\"],\n",
    "#         \"transform_target\": True,\n",
    "        \"experiment_name\": f\"tps_march_2022_{seed}\",\n",
    "        \"silent\": True,\n",
    "#         \"normalize\": True,\n",
    "#         \"transformation\": True,\n",
    "        \"ignore_low_variance\": True,\n",
    "        \"remove_multicollinearity\": True,\n",
    "        \"multicollinearity_threshold\": 0.95,\n",
    "    }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd590c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(log_stdout=True)\n",
    "def train_tps_mar_2022_automl_model(n: int = 1, \n",
    "                                    competition: str = \"tabular-playground-series-mar-2022\", \n",
    "                                    project_name: str = \"kaggle-tps-mar-2022-odsc\"):\n",
    "    now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    for i in range(n):\n",
    "        #Use seed to create a unique configuration for the current pycaret experiment\n",
    "        seed = i + 1 + random.randint(0, 1000)\n",
    "\n",
    "        config = setup_tps_2022_config(seed) #Your specific configs for pycaret data preparation\n",
    "\n",
    "        #Initialize wandb run to begin logging for pycaret experiment\n",
    "        run = wandb.init(project=project_name, reinit=True, config = config,\n",
    "                   name=f\"train-seed-{seed}-{competition}\")\n",
    "        print(f\"Seed: {seed}\")\n",
    "        # run.display(height=360)\n",
    "\n",
    "        #Pull latest training data from wandb and load into df\n",
    "        fe_data_art = run.use_artifact(f\"{project_name}/feature_engineered:latest\", type=competition)\n",
    "        fe_data_path = os.path.join(fe_data_art.download(), \"data\")    \n",
    "        \n",
    "        all_train_data = pd.read_csv(os.path.join(fe_data_path, \"fe_train.csv\"))#.convert_dtypes()\n",
    "        \n",
    "        # train_data = all_train_data.drop([\"time\"], axis=1)\n",
    "        # targetless_data = train_data.drop([\"congestion\"], axis=1)\n",
    "        # corr = pd.DataFrame(np.corrcoef(targetless_data.T))\n",
    "        \n",
    "        train_data = all_train_data[[\"row_id\", \"time\", \"congestion\", \"x\", \"y\", \"direction\"]]\n",
    "        train_data['time'] = pd.to_datetime(train_data['time'])\n",
    "        \n",
    "        #setup and run experiment\n",
    "        #TODO: run with the proper generated features\n",
    "        ts_exp = setup(data=train_data, **config)\n",
    "        model_comparison_results, tuned_finalized_model, tuned_model_results, tuner_cv_results = perform_experiment(ts_exp)\n",
    "\n",
    "                # save model\n",
    "        model_title = f\"{competition}-{seed}\"\n",
    "        save_model(tuned_finalized_model, model_title)\n",
    "        \n",
    "        interpret_model(tuned_finalized_model, save=True)\n",
    "        \n",
    "\n",
    "        # generate wandb tables from the results dfs from our experiment\n",
    "        model_artifacts = wandb.Artifact(\"model_artifacts\", type=competition)\n",
    "\n",
    "        model_comparison_results_table = wandb.Table(dataframe=model_comparison_results.drop([\"Index\", \"Model\", \"Name\"], axis=1))\n",
    "        tuned_model_results_table = wandb.Table(dataframe=tuned_model_results)\n",
    "        tuner_cv_results_table = wandb.Table(dataframe=tuner_cv_results)\n",
    "\n",
    "        # add all objects to artifact\n",
    "        model_artifacts.add(model_comparison_results_table, \"model_comparison_results_table\")\n",
    "        model_artifacts.add(tuned_model_results_table, \"tuned_model_results_table\")\n",
    "        model_artifacts.add(tuner_cv_results_table, \"tuner_cv_results_table\")\n",
    "        model_artifacts.add_file(f\"{model_title}.pkl\", name=\"model.pkl\")\n",
    "        model_artifacts.add_file(\"SHAP summary.png\")\n",
    "\n",
    "        run.log_artifact(model_artifacts)\n",
    "\n",
    "        run.finish()\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _TRAIN:\n",
    "    train_tps_mar_2022_automl_model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f526d9-732d-4986-bb96-82d1f4050fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Log this as wandb run.\n",
    "#TODO: link wandb stuff to here as reference links to make both dashboards easily linkable\n",
    "def configure_prefect_flow():\n",
    "\n",
    "    #TODO: Pass groupid to each run to make it easily searchable\n",
    "    with Flow(\"run-tps-2022-e2e\") as flow:\n",
    "        competition = Parameter(\n",
    "            \"competition\", default=\"tabular-playground-series-mar-2022\")\n",
    "        project_name = Parameter(\n",
    "            \"project_name\", default=\"kaggle-tps-mar-2022-odsc\")\n",
    "        num_experiments = Parameter(\n",
    "            \"num_experiments\", default=3)\n",
    "        \n",
    "        #TODO: Construct prefect graph in a better way\n",
    "        common_params = [competition, project_name]\n",
    "        tasks = [download_and_log_kaggle_data, feature_engineer_log_tps_2022, train_tps_mar_2022_automl_model]\n",
    "        for param in common_params:\n",
    "            for task in tasks:\n",
    "                flow.add_edge(param, task)\n",
    "                \n",
    "        flow.add_edge(download_and_log_kaggle_data, feature_engineer_log_tps_2022)\n",
    "        \n",
    "        flow.add_edge(num_experiments, train_tps_mar_2022_automl_model)\n",
    "        flow.add_edge(feature_engineer_log_tps_2022, train_tps_mar_2022_automl_model)\n",
    "        \n",
    "        # download_and_log_kaggle_data(competition=competition, project_name=project_name)\n",
    "        # feature_engineer_log_tps_2022(competition=competition, project_name=project_name)\n",
    "        # train_tps_mar_2022_automl_model(n=num_experiments, competition=competition, project_name=project_name)\n",
    "        \n",
    "\n",
    "    # Configure the `PROJECT` environment variable for this flow\n",
    "    flow.run_config = LocalRun(\n",
    "        env={\"KAGGLE_USERNAME\": os.environ[\"KAGGLE_USERNAME\"],\n",
    "             \"KAGGLE_KEY\": os.environ[\"KAGGLE_KEY\"], \"WANDB_API_KEY\": os.environ[\"WANDB_API_KEY\"]})\n",
    "\n",
    "    # return flow\n",
    "    flow.register(project_name=\"odsc-east-2022\")\n",
    "    # flow.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _PREFECT_FLOW:\n",
    "    flow = configure_prefect_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd57f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
